#!/usr/bin/env python3
"""
Script pour crÃ©er le dataset final unifiÃ© Ã  partir des chunks
"""

import pandas as pd
import json
from pathlib import Path
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_final_dataset():
    """CrÃ©e le dataset final unifiÃ©"""
    logger.info("ğŸš€ CRÃ‰ATION DU DATASET FINAL")
    logger.info("=" * 50)
    
    # Dossier des chunks
    chunks_dir = Path('data/processed/analyzed_data')
    output_dir = Path('data/processed')
    output_dir.mkdir(exist_ok=True)
    
    # Lire l'index des chunks
    index_file = chunks_dir / 'chunks_index.json'
    with open(index_file, 'r') as f:
        chunks = json.load(f)
    
    logger.info(f"ğŸ“Š Chunks Ã  traiter: {len(chunks)}")
    
    # Collecter tous les DataFrames
    all_dataframes = []
    
    for i, chunk_info in enumerate(chunks):
        try:
            chunk_file = Path(chunk_info['file'])
            logger.info(f"ğŸ“ Traitement {i+1}/{len(chunks)}: {chunk_file.name}")
            
            # Lire le chunk
            df = pd.read_parquet(chunk_file)
            
            # Nettoyage basique
            df = df.dropna(subset=['valeur'])  # Garder seulement les lignes avec des valeurs
            
            if not df.empty:
                all_dataframes.append(df)
                logger.info(f"   âœ… AjoutÃ©: {len(df)} lignes")
            else:
                logger.warning(f"   âš ï¸ Chunk vide aprÃ¨s nettoyage")
                
        except Exception as e:
            logger.error(f"   âŒ Erreur: {e}")
    
    if not all_dataframes:
        logger.error("âŒ Aucun DataFrame valide trouvÃ©")
        return
    
    # Unifier tous les DataFrames
    logger.info("ğŸ”„ Unification des DataFrames...")
    try:
        unified_df = pd.concat(all_dataframes, ignore_index=True, sort=False)
        logger.info(f"âœ… Dataset unifiÃ© crÃ©Ã©: {unified_df.shape}")
        
        # Sauvegarder le dataset final
        output_file = output_dir / 'lumen_final_dataset.parquet'
        unified_df.to_parquet(output_file, index=False)
        logger.info(f"ğŸ’¾ Dataset sauvegardÃ©: {output_file}")
        
        # Afficher un rÃ©sumÃ©
        print_summary(unified_df)
        
        return unified_df
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de l'unification: {e}")
        return None

def print_summary(df):
    """Affiche un rÃ©sumÃ© du dataset final"""
    logger.info("ğŸ“‹ RÃ‰SUMÃ‰ DU DATASET FINAL")
    logger.info("=" * 40)
    logger.info(f"ğŸ“Š Shape: {df.shape}")
    logger.info(f"ğŸ“… Colonnes: {list(df.columns)}")
    
    # Statistiques par source
    if 'source_file' in df.columns:
        logger.info("ğŸ“ Par source:")
        for source in df['source_file'].unique():
            count = len(df[df['source_file'] == source])
            logger.info(f"  - {source}: {count:,} lignes")
    
    # Statistiques par variable
    if 'variable' in df.columns:
        logger.info("ğŸ¯ Par variable:")
        for var in df['variable'].unique():
            count = len(df[df['variable'] == var])
            logger.info(f"  - {var}: {count:,} lignes")
    
    # Statistiques par rÃ©gion
    if 'region' in df.columns:
        logger.info("ğŸ—ºï¸ Par rÃ©gion:")
        for region in df['region'].unique()[:5]:  # Top 5
            count = len(df[df['region'] == region])
            logger.info(f"  - {region}: {count:,} lignes")
    
    logger.info("=" * 40)

if __name__ == "__main__":
    create_final_dataset()
