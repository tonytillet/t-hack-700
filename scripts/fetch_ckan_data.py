#!/usr/bin/env python3
"""
Script de collecte automatis√©e de donn√©es pour LUMEN
Collecte les donn√©es depuis data.gouv.fr, Open-Meteo, Wikipedia, etc.
"""

import requests
import json
import os
import time
from datetime import datetime
import wikipediaapi
from pathlib import Path

class DataCollector:
    def __init__(self):
        self.base_dir = Path("data/raw")
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # Configuration des APIs
        self.data_gouv_url = "https://www.data.gouv.fr/api/1/datasets/"
        self.open_meteo_url = "https://api.open-meteo.com/v1/forecast"
        
        # Configuration Wikipedia
        self.wiki = wikipediaapi.Wikipedia(
            language='fr',
            user_agent='LUMEN-DataCollector/1.0'
        )
        
        # Logs
        self.log_file = self.base_dir / "logs" / f"collect_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        self.log_file.parent.mkdir(exist_ok=True)
    
    def log(self, message):
        """Log un message avec timestamp"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_msg = f"[{timestamp}] {message}"
        print(log_msg)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_msg + '\n')
    
    def collect_data_gouv_fr(self):
        """Collecte les donn√©es depuis data.gouv.fr"""
        self.log("ü¶† Collecte des donn√©es data.gouv.fr...")
        
        # Recherches √† effectuer
        searches = [
            "grippe syndromes grippaux spf",
            "oscour urgences grippe", 
            "vaccination grippe",
            "m√©t√©o france temp√©rature humidit√©",
            "insee population r√©gion",
            "mobilit√© domicile travail",
            "pharmacies officines france",
            "stock vaccin grippe",
            "ansm ventes paracetamol grippe",
            "wikipedia sant√© grippe",
            "twitter sant√© grippe",
            "capacit√© hospitali√®re lits h√¥pital",
            "hopitaux urgences",
            "absent√©isme maladie dares",
            "arr√™t maladie cnam"
        ]
        
        results = {}
        for search in searches:
            try:
                self.log(f"  Recherche: {search}")
                response = requests.get(
                    self.data_gouv_url,
                    params={"q": search, "page_size": 20},
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    results[search] = data
                    
                    # Sauvegarder le r√©sultat
                    filename = search.replace(" ", "_").replace("/", "_") + ".json"
                    filepath = self.base_dir / "data_gouv_fr" / filename
                    filepath.parent.mkdir(exist_ok=True)
                    
                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                    
                    self.log(f"    ‚úÖ Sauvegard√©: {filename} ({len(data.get('data', []))} r√©sultats)")
                else:
                    self.log(f"    ‚ùå Erreur HTTP {response.status_code}")
                    
            except Exception as e:
                self.log(f"    ‚ùå Erreur: {str(e)}")
            
            # Pause entre les requ√™tes
            time.sleep(1)
        
        return results
    
    def collect_weather_data(self):
        """Collecte les donn√©es m√©t√©o depuis Open-Meteo"""
        self.log("üå¶Ô∏è Collecte des donn√©es m√©t√©o...")
        
        # Villes fran√ßaises principales
        cities = [
            {"name": "Paris", "lat": 48.8566, "lon": 2.3522},
            {"name": "Lyon", "lat": 45.7640, "lon": 4.8357},
            {"name": "Marseille", "lat": 43.2965, "lon": 5.3698},
            {"name": "Toulouse", "lat": 43.6047, "lon": 1.4442},
            {"name": "Nice", "lat": 43.7102, "lon": 7.2620}
        ]
        
        weather_data = {}
        
        for city in cities:
            try:
                self.log(f"  M√©t√©o pour {city['name']}...")
                
                response = requests.get(
                    self.open_meteo_url,
                    params={
                        "latitude": city["lat"],
                        "longitude": city["lon"],
                        "current": "temperature_2m,relative_humidity_2m,precipitation",
                        "hourly": "temperature_2m,relative_humidity_2m,precipitation",
                        "daily": "temperature_2m_max,temperature_2m_min,precipitation_sum",
                        "timezone": "Europe/Paris",
                        "forecast_days": 7
                    },
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    weather_data[city["name"]] = data
                    self.log(f"    ‚úÖ Donn√©es m√©t√©o r√©cup√©r√©es pour {city['name']}")
                else:
                    self.log(f"    ‚ùå Erreur HTTP {response.status_code}")
                    
            except Exception as e:
                self.log(f"    ‚ùå Erreur m√©t√©o {city['name']}: {str(e)}")
            
            time.sleep(0.5)
        
        # Sauvegarder les donn√©es m√©t√©o
        filepath = self.base_dir / "other" / "donnees_meteo.json"
        filepath.parent.mkdir(exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(weather_data, f, ensure_ascii=False, indent=2)
        
        self.log(f"‚úÖ Donn√©es m√©t√©o sauvegard√©es: {filepath}")
        return weather_data
    
    def collect_wikipedia_data(self):
        """Collecte les donn√©es depuis Wikipedia"""
        self.log("üß† Collecte des donn√©es Wikipedia...")
        
        # Pages Wikipedia sant√© √† r√©cup√©rer
        pages = [
            "Grippe",
            "√âpid√©mie de grippe",
            "Vaccin antigrippal",
            "Sant√© publique en France",
            "Institut Pasteur",
            "Sant√© publique France",
            "Agence nationale de s√©curit√© du m√©dicament",
            "Direction de la recherche, des √©tudes, de l'√©valuation et des statistiques",
            "Minist√®re de la Sant√© (France)",
            "Syst√®me de sant√© fran√ßais",
            "Pr√©vention des maladies",
            "Surveillance √©pid√©miologique"
        ]
        
        wiki_data = {}
        
        for page_title in pages:
            try:
                self.log(f"  Page: {page_title}")
                page = self.wiki.page(page_title)
                
                if page.exists():
                    wiki_data[page_title] = {
                        "title": page.title,
                        "summary": page.summary,
                        "text": page.text[:5000],  # Limiter la taille
                        "url": page.fullurl,
                        "categories": list(page.categories.keys())[:10]  # Limiter les cat√©gories
                    }
                    self.log(f"    ‚úÖ Page r√©cup√©r√©e: {page_title}")
                else:
                    self.log(f"    ‚ùå Page non trouv√©e: {page_title}")
                    
            except Exception as e:
                self.log(f"    ‚ùå Erreur Wikipedia {page_title}: {str(e)}")
            
            time.sleep(1)
        
        # Sauvegarder les donn√©es Wikipedia
        filepath = self.base_dir / "other" / "wikipedia_sante.json"
        filepath.parent.mkdir(exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(wiki_data, f, ensure_ascii=False, indent=2)
        
        self.log(f"‚úÖ Donn√©es Wikipedia sauvegard√©es: {filepath}")
        return wiki_data
    
    def collect_insee_data(self):
        """Collecte les donn√©es INSEE (structure seulement)"""
        self.log("üë• Collecte des donn√©es INSEE...")
        
        try:
            # Test de l'API MELODI INSEE
            melodi_url = "https://api.insee.fr/metadonnees/V1/codes/cog/communes"
            
            response = requests.get(melodi_url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                
                # Sauvegarder la structure
                filepath = self.base_dir / "insee" / "melodi_structure.json"
                filepath.parent.mkdir(exist_ok=True)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                self.log(f"‚úÖ Structure INSEE sauvegard√©e: {filepath}")
                return data
            else:
                self.log(f"‚ùå Erreur INSEE HTTP {response.status_code}")
                
        except Exception as e:
            self.log(f"‚ùå Erreur INSEE: {str(e)}")
        
        return {}
    
    def create_summary(self, all_data):
        """Cr√©e un r√©sum√© de toutes les donn√©es collect√©es"""
        self.log("üìä Cr√©ation du r√©sum√©...")
        
        summary = {
            "timestamp": datetime.now().isoformat(),
            "total_sources": len(all_data),
            "sources": {},
            "statistics": {
                "total_files": 0,
                "total_size_mb": 0,
                "success_rate": 0
            }
        }
        
        # Analyser chaque source
        for source, data in all_data.items():
            if isinstance(data, dict):
                summary["sources"][source] = {
                    "type": "dict",
                    "keys": len(data.keys()),
                    "status": "success"
                }
            elif isinstance(data, list):
                summary["sources"][source] = {
                    "type": "list", 
                    "items": len(data),
                    "status": "success"
                }
        
        # Sauvegarder le r√©sum√©
        filepath = self.base_dir / "summary.json"
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.log(f"‚úÖ R√©sum√© cr√©√©: {filepath}")
        return summary
    
    def run_collection(self):
        """Lance la collecte compl√®te"""
        self.log("üöÄ D√âBUT DE LA COLLECTE DE DONN√âES LUMEN")
        self.log("=" * 50)
        
        all_data = {}
        
        try:
            # 1. Collecte data.gouv.fr
            data_gouv_results = self.collect_data_gouv_fr()
            all_data["data_gouv_fr"] = data_gouv_results
            
            # 2. Collecte m√©t√©o
            weather_data = self.collect_weather_data()
            all_data["weather"] = weather_data
            
            # 3. Collecte Wikipedia
            wiki_data = self.collect_wikipedia_data()
            all_data["wikipedia"] = wiki_data
            
            # 4. Collecte INSEE
            insee_data = self.collect_insee_data()
            all_data["insee"] = insee_data
            
            # 5. Cr√©er le r√©sum√©
            summary = self.create_summary(all_data)
            all_data["summary"] = summary
            
            self.log("=" * 50)
            self.log("‚úÖ COLLECTE TERMIN√âE AVEC SUCC√àS")
            self.log(f"üìä R√©sum√© sauvegard√© dans: {self.log_file}")
            
        except Exception as e:
            self.log(f"‚ùå ERREUR CRITIQUE: {str(e)}")
            raise
        
        return all_data

def main():
    """Point d'entr√©e principal"""
    print("üîç LUMEN - Collecte de donn√©es automatis√©e")
    print("=" * 50)
    
    collector = DataCollector()
    results = collector.run_collection()
    
    print("\nüéØ COLLECTE TERMIN√âE")
    print("=" * 50)
    print(f"üìÅ Donn√©es sauvegard√©es dans: data/raw/")
    print(f"üìù Logs disponibles dans: {collector.log_file}")
    print(f"üìä R√©sum√©: {len(results)} sources collect√©es")

if __name__ == "__main__":
    main()
