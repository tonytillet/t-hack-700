#!/usr/bin/env python3
"""
Script simple pour extraire les sÃ©ries temporelles des donnÃ©es urgences
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime, timedelta
import json

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleTimeSeriesExtractor:
    def __init__(self):
        self.base_dir = Path("data")
        self.emergency_dir = self.base_dir / "processed" / "emergency_data"
        self.timeseries_dir = self.base_dir / "processed" / "timeseries"
        self.timeseries_dir.mkdir(parents=True, exist_ok=True)
    
    def load_and_parse_data(self):
        """Charge et parse les donnÃ©es urgences"""
        logger.info("ğŸ“Š CHARGEMENT ET PARSING DES DONNÃ‰ES")
        logger.info("=" * 50)
        
        try:
            emergency_file = self.emergency_dir / "emergency_data.parquet"
            df = pd.read_parquet(emergency_file)
            
            logger.info(f"ğŸ“Š Shape: {df.shape}")
            
            # Chercher les colonnes avec des donnÃ©es structurÃ©es
            structured_data = []
            
            # Colonnes avec donnÃ©es sÃ©parÃ©es par point-virgule
            semicolon_cols = [col for col in df.columns if ';' in str(col)]
            logger.info(f"ğŸ“… Colonnes avec point-virgule: {len(semicolon_cols)}")
            
            for col in semicolon_cols:
                logger.info(f"ğŸ” Analyse de: {col[:50]}...")
                
                # Prendre un Ã©chantillon pour analyser la structure
                sample_values = df[col].dropna().head(10).tolist()
                logger.info(f"ğŸ“Š Ã‰chantillon: {sample_values[:3]}")
                
                # Parser les donnÃ©es si elles contiennent des informations utiles
                for idx, value in enumerate(df[col].dropna()):
                    if idx % 100000 == 0:
                        logger.info(f"ğŸ“Š Traitement ligne {idx}")
                    
                    try:
                        # Split par point-virgule
                        parts = str(value).split(';')
                        
                        if len(parts) >= 3:
                            # Essayer d'extraire date, dÃ©partement, et cas COVID
                            date_str = parts[1] if len(parts) > 1 else None
                            dep_code = parts[0] if len(parts) > 0 else None
                            covid_cases = parts[2] if len(parts) > 2 else None
                            
                            # Valider et convertir
                            if date_str and dep_code and covid_cases:
                                try:
                                    date_val = pd.to_datetime(date_str, errors='coerce')
                                    dep_val = str(dep_code).strip()
                                    covid_val = float(covid_cases)
                                    
                                    if (pd.notna(date_val) and 
                                        date_val.year >= 2020 and 
                                        len(dep_val) == 2 and 
                                        covid_val >= 0):
                                        
                                        structured_data.append({
                                            'date': date_val,
                                            'department': dep_val,
                                            'covid_cases': covid_val
                                        })
                                        
                                        if len(structured_data) % 10000 == 0:
                                            logger.info(f"âœ… {len(structured_data)} enregistrements extraits")
                                            
                                except:
                                    continue
                    
                    except Exception as e:
                        continue
            
            if structured_data:
                clean_df = pd.DataFrame(structured_data)
                logger.info(f"âœ… DonnÃ©es structurÃ©es extraites: {clean_df.shape}")
                logger.info(f"ğŸ“… PÃ©riode: {clean_df['date'].min()} Ã  {clean_df['date'].max()}")
                logger.info(f"ğŸ—ºï¸ DÃ©partements: {clean_df['department'].nunique()}")
                return clean_df
            else:
                logger.warning("âš ï¸ Aucune donnÃ©e structurÃ©e trouvÃ©e")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Erreur parsing: {e}")
            return None
    
    def create_daily_series(self, df):
        """CrÃ©e les sÃ©ries temporelles quotidiennes"""
        logger.info("ğŸ“… CRÃ‰ATION DES SÃ‰RIES QUOTIDIENNES")
        logger.info("=" * 50)
        
        try:
            # AgrÃ©ger par date et dÃ©partement
            daily_series = df.groupby(['date', 'department']).agg({
                'covid_cases': 'sum'
            }).reset_index()
            
            # Ajouter des features temporelles
            daily_series['year'] = daily_series['date'].dt.year
            daily_series['month'] = daily_series['date'].dt.month
            daily_series['day_of_week'] = daily_series['date'].dt.dayofweek
            daily_series['week_of_year'] = daily_series['date'].dt.isocalendar().week
            daily_series['quarter'] = daily_series['date'].dt.quarter
            
            # Features saisonniÃ¨res
            daily_series['is_weekend'] = daily_series['day_of_week'].isin([5, 6]).astype(int)
            daily_series['is_peak_season'] = daily_series['month'].isin([10, 11, 12, 1, 2, 3]).astype(int)
            
            # Moyennes mobiles
            daily_series = daily_series.sort_values(['department', 'date'])
            daily_series['covid_cases_ma7'] = daily_series.groupby('department')['covid_cases'].rolling(window=7, min_periods=1).mean().reset_index(0, drop=True)
            daily_series['covid_cases_ma30'] = daily_series.groupby('department')['covid_cases'].rolling(window=30, min_periods=1).mean().reset_index(0, drop=True)
            
            logger.info(f"âœ… SÃ©ries quotidiennes: {daily_series.shape}")
            logger.info(f"ğŸ“… PÃ©riode: {daily_series['date'].min()} Ã  {daily_series['date'].max()}")
            logger.info(f"ğŸ—ºï¸ DÃ©partements: {daily_series['department'].nunique()}")
            
            return daily_series
            
        except Exception as e:
            logger.error(f"âŒ Erreur crÃ©ation sÃ©ries: {e}")
            return None
    
    def create_target_and_features(self, daily_series):
        """CrÃ©e la target variable et les features"""
        logger.info("ğŸ¯ CRÃ‰ATION TARGET ET FEATURES")
        logger.info("=" * 50)
        
        try:
            # Target variable (urgences dans 7 jours)
            target_series = daily_series.copy()
            target_series['target_date'] = target_series['date'] + timedelta(days=7)
            target_series = target_series.rename(columns={'covid_cases': 'target_covid_cases'})
            
            # Features avancÃ©es
            features_series = daily_series.copy()
            
            # Tendance
            features_series['covid_trend_7d'] = features_series.groupby('department')['covid_cases'].pct_change(7)
            features_series['covid_trend_30d'] = features_series.groupby('department')['covid_cases'].pct_change(30)
            
            # VolatilitÃ©
            features_series['covid_volatility_7d'] = features_series.groupby('department')['covid_cases'].rolling(window=7, min_periods=1).std().reset_index(0, drop=True)
            features_series['covid_volatility_30d'] = features_series.groupby('department')['covid_cases'].rolling(window=30, min_periods=1).std().reset_index(0, drop=True)
            
            # SaisonnalitÃ©
            features_series['sin_month'] = np.sin(2 * np.pi * features_series['month'] / 12)
            features_series['cos_month'] = np.cos(2 * np.pi * features_series['month'] / 12)
            features_series['sin_day'] = np.sin(2 * np.pi * features_series['day_of_week'] / 7)
            features_series['cos_day'] = np.cos(2 * np.pi * features_series['day_of_week'] / 7)
            
            # Interactions
            features_series['covid_weekend_interaction'] = features_series['covid_cases'] * features_series['is_weekend']
            features_series['covid_peak_interaction'] = features_series['covid_cases'] * features_series['is_peak_season']
            
            logger.info(f"âœ… Target: {target_series.shape}")
            logger.info(f"âœ… Features: {features_series.shape}")
            
            return target_series, features_series
            
        except Exception as e:
            logger.error(f"âŒ Erreur crÃ©ation target/features: {e}")
            return None, None
    
    def save_series(self, daily_series, target_series, features_series):
        """Sauvegarde toutes les sÃ©ries"""
        logger.info("ğŸ’¾ SAUVEGARDE DES SÃ‰RIES")
        logger.info("=" * 50)
        
        try:
            # Sauvegarder
            daily_series.to_parquet(self.timeseries_dir / "daily_emergency_series_simple.parquet", index=False)
            target_series.to_parquet(self.timeseries_dir / "target_series_simple.parquet", index=False)
            features_series.to_parquet(self.timeseries_dir / "features_series_simple.parquet", index=False)
            
            # RÃ©sumÃ©
            summary = {
                "daily_series": {
                    "shape": daily_series.shape,
                    "period": f"{daily_series['date'].min()} to {daily_series['date'].max()}",
                    "departments": daily_series['department'].nunique()
                },
                "target_series": {
                    "shape": target_series.shape,
                    "period": f"{target_series['date'].min()} to {target_series['date'].max()}"
                },
                "features_series": {
                    "shape": features_series.shape,
                    "period": f"{features_series['date'].min()} to {features_series['date'].max()}"
                }
            }
            
            with open(self.timeseries_dir / "timeseries_summary_simple.json", 'w') as f:
                json.dump(summary, f, indent=2, default=str)
            
            logger.info("âœ… SÃ‰RIES SAUVEGARDÃ‰ES")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur sauvegarde: {e}")
            return False
    
    def run_extraction(self):
        """Lance l'extraction des sÃ©ries temporelles"""
        logger.info("ğŸš€ DÃ‰BUT DE L'EXTRACTION DES SÃ‰RIES TEMPORELLES")
        logger.info("=" * 60)
        
        # Charger et parser les donnÃ©es
        df = self.load_and_parse_data()
        if df is None:
            return False
        
        # CrÃ©er les sÃ©ries quotidiennes
        daily_series = self.create_daily_series(df)
        if daily_series is None:
            return False
        
        # CrÃ©er target et features
        target_series, features_series = self.create_target_and_features(daily_series)
        if target_series is None or features_series is None:
            return False
        
        # Sauvegarder
        success = self.save_series(daily_series, target_series, features_series)
        
        if success:
            logger.info("âœ… EXTRACTION TERMINÃ‰E")
            logger.info("=" * 60)
            return True
        else:
            logger.error("âŒ Ã‰CHEC DE L'EXTRACTION")
            return False

if __name__ == "__main__":
    extractor = SimpleTimeSeriesExtractor()
    extractor.run_extraction()
