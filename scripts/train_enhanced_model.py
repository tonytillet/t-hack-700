#!/usr/bin/env python3
"""
Script d'entraÃ®nement du modÃ¨le amÃ©liorÃ© avec features temporelles
Utilise la comparaison inter-annÃ©es (N-2, N-1, N) pour prÃ©dire N+1
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import joblib
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class EnhancedFluPredictor:
    def __init__(self):
        """Initialise le prÃ©dicteur de grippe amÃ©liorÃ©"""
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = []
        self.target_columns = []
        self.feature_importance = None
        
    def load_enhanced_data(self, filepath):
        """Charge le dataset amÃ©liorÃ© avec features temporelles"""
        print(f"ğŸ“‚ Chargement du dataset amÃ©liorÃ©: {filepath}")
        
        self.data = pd.read_csv(filepath)
        self.data['date'] = pd.to_datetime(self.data['date'])
        
        print(f"  âœ… Dataset chargÃ©: {len(self.data)} enregistrements, {len(self.data.columns)} colonnes")
        print(f"  ğŸ“… PÃ©riode: {self.data['date'].min().strftime('%Y-%m-%d')} Ã  {self.data['date'].max().strftime('%Y-%m-%d')}")
        
        return self.data
    
    def prepare_enhanced_features(self):
        """PrÃ©pare les features amÃ©liorÃ©es avec comparaison inter-annÃ©es"""
        print("\nğŸ”§ PrÃ©paration des features amÃ©liorÃ©es...")
        
        df = self.data.copy()
        df = df.sort_values(['region', 'date']).reset_index(drop=True)
        
        # Features de base
        base_features = [
            'urgences_grippe', 'cas_sentinelles', 'ias_syndrome_grippal',
            'google_trends_grippe', 'google_trends_vaccin', 'google_trends_symptomes',
            'wiki_grippe_views', 'wiki_vaccination_views',
            'temperature', 'humidity', 'taux_vaccination', 'population_65_plus_pct'
        ]
        
        # Features temporelles de base
        temporal_features = []
        for feature in base_features:
            if feature in df.columns:
                # Lags 1-4 semaines
                for lag in range(1, 5):
                    df[f'{feature}_lag_{lag}'] = df.groupby('region')[feature].shift(lag)
                    temporal_features.append(f'{feature}_lag_{lag}')
                
                # Moyennes mobiles 2-4 semaines
                for window in [2, 4]:
                    df[f'{feature}_ma_{window}'] = df.groupby('region')[feature].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)
                    temporal_features.append(f'{feature}_ma_{window}')
        
        # Features de comparaison inter-annÃ©es
        yearly_features = []
        for feature in ['urgences_grippe', 'cas_sentinelles', 'ias_syndrome_grippal']:
            if feature in df.columns:
                # Features N-2, N-1, N
                yearly_features.extend([
                    f'{feature}_year_minus_2', f'{feature}_year_minus_1', f'{feature}_year_current',
                    f'{feature}_diff_n1_n2', f'{feature}_diff_n_n1', f'{feature}_diff_n_n2',
                    f'{feature}_ratio_n_n1', f'{feature}_ratio_n_n2',
                    f'{feature}_mean_3years', f'{feature}_std_3years', f'{feature}_zscore_3years'
                ])
        
        # Features saisonniÃ¨res
        seasonal_features = []
        for feature in ['urgences_grippe', 'cas_sentinelles']:
            if f'{feature}_seasonal_anomaly' in df.columns:
                seasonal_features.append(f'{feature}_seasonal_anomaly')
        
        # Features d'Ã©pidÃ©mie
        epidemic_features = []
        for feature in ['urgences_grippe', 'cas_sentinelles']:
            if f'{feature}_epidemic_level' in df.columns:
                epidemic_features.extend([
                    f'{feature}_epidemic_level', f'{feature}_distance_to_epidemic', f'{feature}_epidemic_probability'
                ])
        
        # Features de tendance
        trend_features = []
        for feature in ['urgences_grippe', 'cas_sentinelles']:
            if f'{feature}_trend' in df.columns:
                trend_features.extend([f'{feature}_trend', f'{feature}_trend_ratio'])
        
        # Features mÃ©tÃ©o et dÃ©mographiques
        context_features = ['temperature', 'humidity', 'taux_vaccination', 'population_65_plus_pct']
        
        # Features Google Trends et Wikipedia
        trends_features = ['google_trends_grippe', 'google_trends_vaccin', 'google_trends_symptomes']
        wiki_features = ['wiki_grippe_views', 'wiki_vaccination_views']
        
        # Combinaison de toutes les features
        all_features = (base_features + temporal_features + yearly_features + 
                       seasonal_features + epidemic_features + trend_features + 
                       context_features + trends_features + wiki_features)
        
        # Filtrage des features disponibles
        available_features = [f for f in all_features if f in df.columns]
        
        print(f"  ğŸ“Š Features disponibles: {len(available_features)}")
        print(f"    - Features de base: {len([f for f in base_features if f in df.columns])}")
        print(f"    - Features temporelles: {len(temporal_features)}")
        print(f"    - Features inter-annÃ©es: {len(yearly_features)}")
        print(f"    - Features saisonniÃ¨res: {len(seasonal_features)}")
        print(f"    - Features d'Ã©pidÃ©mie: {len(epidemic_features)}")
        print(f"    - Features de tendance: {len(trend_features)}")
        
        self.feature_columns = available_features
        return df
    
    def create_multi_horizon_targets(self, df):
        """CrÃ©e les targets multi-horizons (J+1, J+7, J+14, J+21, J+28)"""
        print("\nğŸ¯ CrÃ©ation des targets multi-horizons...")
        
        # Targets pour les urgences grippe
        if 'urgences_grippe' in df.columns:
            df['target_urgences_j1'] = df.groupby('region')['urgences_grippe'].shift(-1)
            df['target_urgences_j7'] = df.groupby('region')['urgences_grippe'].shift(-7)
            df['target_urgences_j14'] = df.groupby('region')['urgences_grippe'].shift(-14)
            df['target_urgences_j21'] = df.groupby('region')['urgences_grippe'].shift(-21)
            df['target_urgences_j28'] = df.groupby('region')['urgences_grippe'].shift(-28)
            
            self.target_columns = ['target_urgences_j1', 'target_urgences_j7', 'target_urgences_j14', 'target_urgences_j21', 'target_urgences_j28']
            print(f"  âœ… Targets crÃ©Ã©s: {len(self.target_columns)} horizons")
        
        return df
    
    def prepare_training_data(self, df):
        """PrÃ©pare les donnÃ©es d'entraÃ®nement"""
        print("\nğŸ“Š PrÃ©paration des donnÃ©es d'entraÃ®nement...")
        
        # Suppression des lignes avec valeurs manquantes
        df_clean = df.dropna(subset=self.feature_columns + self.target_columns)
        
        print(f"  ğŸ“Š DonnÃ©es aprÃ¨s nettoyage: {len(df_clean)} enregistrements")
        
        # SÃ©paration des features et targets
        X = df_clean[self.feature_columns]
        y = df_clean[self.target_columns]
        
        # Normalisation des features
        X_scaled = self.scaler.fit_transform(X)
        
        print(f"  âœ… Features normalisÃ©es: {X_scaled.shape}")
        print(f"  âœ… Targets prÃ©parÃ©s: {y.shape}")
        
        return X_scaled, y, df_clean
    
    def train_enhanced_model(self, X, y):
        """EntraÃ®ne le modÃ¨le Random Forest amÃ©liorÃ©"""
        print("\nğŸ¤– EntraÃ®nement du modÃ¨le Random Forest amÃ©liorÃ©...")
        
        # Configuration du modÃ¨le
        rf_params = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, 30, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', None],
            'bootstrap': [True, False]
        }
        
        # Validation croisÃ©e temporelle
        tscv = TimeSeriesSplit(n_splits=5)
        
        # Recherche d'hyperparamÃ¨tres
        rf = RandomForestRegressor(random_state=42, n_jobs=-1)
        random_search = RandomizedSearchCV(
            rf, rf_params, n_iter=50, cv=tscv, 
            scoring='neg_mean_absolute_error', 
            random_state=42, n_jobs=-1
        )
        
        # EntraÃ®nement
        print("  ğŸ”„ Recherche des meilleurs hyperparamÃ¨tres...")
        random_search.fit(X, y)
        
        self.model = random_search.best_estimator_
        self.feature_importance = self.model.feature_importances_
        
        print(f"  âœ… Meilleurs paramÃ¨tres: {random_search.best_params_}")
        print(f"  âœ… Score CV: {-random_search.best_score_:.2f} MAE")
        
        return self.model
    
    def evaluate_model(self, X, y, df_clean):
        """Ã‰value le modÃ¨le sur les donnÃ©es de test"""
        print("\nğŸ“Š Ã‰valuation du modÃ¨le...")
        
        # PrÃ©dictions
        y_pred = self.model.predict(X)
        
        # MÃ©triques par horizon
        results = {}
        for i, target in enumerate(self.target_columns):
            mae = mean_absolute_error(y.iloc[:, i], y_pred[:, i])
            r2 = r2_score(y.iloc[:, i], y_pred[:, i])
            
            results[target] = {'MAE': mae, 'RÂ²': r2}
            
            print(f"  ğŸ“ˆ {target}: MAE={mae:.2f}, RÂ²={r2:.3f}")
        
        # Importance des features
        feature_importance_df = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.feature_importance
        }).sort_values('importance', ascending=False)
        
        print(f"\nğŸ” Top 10 des features les plus importantes:")
        for i, row in feature_importance_df.head(10).iterrows():
            print(f"  {row['feature']}: {row['importance']:.3f}")
        
        return results, feature_importance_df
    
    def save_model(self, model_path=None):
        """Sauvegarde le modÃ¨le entraÃ®nÃ©"""
        if model_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            model_path = f'models/flu_predictor_enhanced_{timestamp}.joblib'
        
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        # Sauvegarde du modÃ¨le et du scaler
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'target_columns': self.target_columns,
            'feature_importance': self.feature_importance
        }
        
        joblib.dump(model_data, model_path)
        print(f"ğŸ’¾ ModÃ¨le sauvegardÃ©: {model_path}")
        
        return model_path

def main():
    """Fonction principale"""
    print("ğŸš€ ENTRAÃNEMENT DU MODÃˆLE AMÃ‰LIORÃ‰")
    print("=" * 60)
    
    # Chargement du dataset amÃ©liorÃ©
    dataset_file = 'data/processed/dataset_grippe_enhanced_20251020_195427.csv'
    if not os.path.exists(dataset_file):
        print(f"âŒ Dataset amÃ©liorÃ© non trouvÃ©: {dataset_file}")
        return
    
    # Initialisation
    predictor = EnhancedFluPredictor()
    df = predictor.load_enhanced_data(dataset_file)
    
    # PrÃ©paration des features
    df = predictor.prepare_enhanced_features()
    
    # CrÃ©ation des targets
    df = predictor.create_multi_horizon_targets(df)
    
    # PrÃ©paration des donnÃ©es d'entraÃ®nement
    X, y, df_clean = predictor.prepare_training_data(df)
    
    # EntraÃ®nement du modÃ¨le
    model = predictor.train_enhanced_model(X, y)
    
    # Ã‰valuation
    results, feature_importance = predictor.evaluate_model(X, y, df_clean)
    
    # Sauvegarde
    model_path = predictor.save_model()
    
    print(f"\nâœ… ENTRAÃNEMENT TERMINÃ‰")
    print(f"ğŸ“ ModÃ¨le sauvegardÃ©: {model_path}")
    print(f"ğŸ“Š Performance moyenne MAE: {np.mean([r['MAE'] for r in results.values()]):.2f}")
    print(f"ğŸ“Š Performance moyenne RÂ²: {np.mean([r['RÂ²'] for r in results.values()]):.3f}")

if __name__ == "__main__":
    main()
