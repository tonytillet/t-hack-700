#!/usr/bin/env python3
"""
Script pour t√©l√©charger les ressources urgences trouv√©es
"""

import pandas as pd
import requests
import json
import io
from pathlib import Path
import logging
from datetime import datetime
import time

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EmergencyResourceDownloader:
    def __init__(self):
        self.base_dir = Path("data")
        self.emergency_dir = self.base_dir / "processed" / "emergency_data"
        self.emergency_dir.mkdir(parents=True, exist_ok=True)
        
        # Cl√© API fournie
        self.api_key = "eyJhbGciOiJIUzUxMiJ9.eyJ1c2VyIjoiNjhmZDQwYTRkMjkwNjY3Njc2Y2YzMTVhIiwidGltZSI6MTc2MTQyNzgwNi4yNjc5OTA4fQ.0RMs5OiUW1-Sg9Y7ONe-__kCJga3mP3u2sTgpIEVpCp8rH3aWHf9tS6U-VU2sMi3f7oNgUUw-Eev-ejQ6zxDDg"
        
        # Headers avec authentification
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Accept": "application/json",
            "Content-Type": "application/json"
        }
    
    def load_search_results(self):
        """Charge les r√©sultats de recherche"""
        logger.info("üìä CHARGEMENT DES R√âSULTATS DE RECHERCHE")
        logger.info("=" * 50)
        
        try:
            # Lire les r√©sultats urgences
            emergency_file = self.emergency_dir / "emergency_search_results.json"
            with open(emergency_file, 'r', encoding='utf-8') as f:
                emergency_results = json.load(f)
            
            logger.info(f"‚úÖ R√©sultats urgences charg√©s: {len(emergency_results)} datasets")
            return emergency_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement r√©sultats: {e}")
            return []
    
    def download_emergency_resources(self, datasets):
        """T√©l√©charge les ressources urgences"""
        logger.info("üì• T√âL√âCHARGEMENT DES RESSOURCES URGENCES")
        logger.info("=" * 50)
        
        all_emergency_data = []
        
        for i, dataset in enumerate(datasets):
            try:
                logger.info(f"üìä Dataset {i+1}/{len(datasets)}: {dataset['title']}")
                
                for j, resource in enumerate(dataset['resources']):
                    if resource.get('format') in ['csv', 'json'] and resource.get('url'):
                        try:
                            logger.info(f"  üì• Ressource {j+1}: {resource['url']}")
                            
                            # T√©l√©charger la ressource
                            response = requests.get(resource['url'], timeout=60)
                            
                            if response.status_code == 200:
                                # Traiter selon le format
                                if resource['format'] == 'csv':
                                    df = pd.read_csv(io.StringIO(response.text))
                                elif resource['format'] == 'json':
                                    json_data = response.json()
                                    if isinstance(json_data, list):
                                        df = pd.DataFrame(json_data)
                                    elif isinstance(json_data, dict):
                                        df = pd.DataFrame([json_data])
                                    else:
                                        continue
                                
                                # Ajouter des m√©tadonn√©es
                                df['source_dataset'] = dataset['title']
                                df['source_organization'] = dataset['organization']
                                df['resource_url'] = resource['url']
                                df['download_date'] = datetime.now().isoformat()
                                
                                all_emergency_data.append(df)
                                logger.info(f"    ‚úÖ Ajout√©: {len(df)} lignes")
                                
                            else:
                                logger.warning(f"    ‚ùå Erreur t√©l√©chargement: {response.status_code}")
                        
                        except Exception as e:
                            logger.warning(f"    ‚ùå Erreur ressource: {e}")
                
                # Pause entre les datasets
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur dataset: {e}")
        
        # Sauvegarder les donn√©es urgences
        if all_emergency_data:
            emergency_df = pd.concat(all_emergency_data, ignore_index=True, sort=False)
            emergency_file = self.emergency_dir / "emergency_data.parquet"
            emergency_df.to_parquet(emergency_file, index=False)
            logger.info(f"üíæ Donn√©es urgences sauvegard√©es: {emergency_file}")
            logger.info(f"üìä Total lignes: {len(emergency_df)}")
            return emergency_df
        
        return None
    
    def analyze_emergency_data(self, df):
        """Analyse les donn√©es urgences t√©l√©charg√©es"""
        logger.info("üîç ANALYSE DES DONN√âES URGENCES")
        logger.info("=" * 50)
        
        try:
            logger.info(f"üìä Shape: {df.shape}")
            logger.info(f"üìÖ Colonnes: {list(df.columns)}")
            
            # Analyser les colonnes temporelles
            date_cols = [col for col in df.columns if 'date' in col.lower() or 'jour' in col.lower() or 'time' in col.lower()]
            logger.info(f"üìÖ Colonnes temporelles: {date_cols}")
            
            # Analyser les colonnes g√©ographiques
            geo_cols = [col for col in df.columns if 'dep' in col.lower() or 'region' in col.lower() or 'dept' in col.lower()]
            logger.info(f"üó∫Ô∏è Colonnes g√©ographiques: {geo_cols}")
            
            # Analyser les colonnes de sant√©
            health_cols = [col for col in df.columns if 'urg' in col.lower() or 'grippe' in col.lower() or 'covid' in col.lower() or 'syndrome' in col.lower()]
            logger.info(f"üè• Colonnes sant√©: {health_cols}")
            
            # Statistiques par source
            if 'source_dataset' in df.columns:
                logger.info("üìÅ Par source:")
                for source in df['source_dataset'].unique():
                    count = len(df[df['source_dataset'] == source])
                    logger.info(f"  - {source}: {count:,} lignes")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse: {e}")
            return False
    
    def run_download(self):
        """Lance le t√©l√©chargement des ressources"""
        logger.info("üöÄ D√âBUT DU T√âL√âCHARGEMENT DES RESSOURCES")
        logger.info("=" * 60)
        
        # Charger les r√©sultats de recherche
        datasets = self.load_search_results()
        
        if not datasets:
            logger.error("‚ùå Aucun r√©sultat de recherche trouv√©")
            return None
        
        # T√©l√©charger les ressources
        emergency_df = self.download_emergency_resources(datasets)
        
        if emergency_df is not None:
            # Analyser les donn√©es
            self.analyze_emergency_data(emergency_df)
            logger.info("‚úÖ T√âL√âCHARGEMENT TERMIN√â")
            logger.info("=" * 60)
            return emergency_df
        else:
            logger.warning("‚ö†Ô∏è Aucune donn√©e urgence t√©l√©charg√©e")
            return None

if __name__ == "__main__":
    downloader = EmergencyResourceDownloader()
    downloader.run_download()
