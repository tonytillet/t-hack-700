#!/usr/bin/env python3
"""
Script pour calculer les statistiques de r√©f√©rence (medians.json et cats.json)
√† partir des fichiers Parquet
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
import json
from datetime import datetime

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StatsFitter:
    def __init__(self):
        self.base_dir = Path("data")
        self.processed_dir = self.base_dir / "processed"
        self.timeseries_dir = self.processed_dir / "timeseries"
        self.stats_dir = self.base_dir / "config"
        self.stats_dir.mkdir(parents=True, exist_ok=True)
        
        # Fichiers de sortie
        self.medians_path = self.stats_dir / "medians.json"
        self.cats_path = self.stats_dir / "cats.json"
    
    def load_timeseries_data(self):
        """Charge les donn√©es des s√©ries temporelles"""
        logger.info("üìä CHARGEMENT DES DONN√âES S√âRIES TEMPORELLES")
        logger.info("=" * 50)
        
        try:
            # Charger les s√©ries quotidiennes
            daily_file = self.timeseries_dir / "daily_emergency_series_simple.parquet"
            if not daily_file.exists():
                logger.error(f"‚ùå Fichier non trouv√©: {daily_file}")
                return None
            
            df = pd.read_parquet(daily_file)
            logger.info(f"‚úÖ Donn√©es charg√©es: {df.shape}")
            logger.info(f"üìÖ P√©riode: {df['date'].min()} √† {df['date'].max()}")
            logger.info(f"üó∫Ô∏è D√©partements: {df['department'].nunique()}")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement: {e}")
            return None
    
    def calculate_medians(self, df):
        """Calcule les m√©dianes pour les variables num√©riques"""
        logger.info("üìä CALCUL DES M√âDIANES")
        logger.info("=" * 50)
        
        try:
            # Identifier les colonnes num√©riques
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            logger.info(f"üìä Colonnes num√©riques: {numeric_cols}")
            
            # Calculer les m√©dianes
            medians = {}
            for col in numeric_cols:
                if col not in ['year', 'month', 'day_of_week', 'week_of_year', 'quarter']:
                    median_val = df[col].median()
                    medians[col] = float(median_val) if not pd.isna(median_val) else 0.0
                    logger.info(f"  - {col}: {medians[col]:.2f}")
            
            # Ajouter des m√©dianes par d√©partement pour les variables cl√©s
            key_vars = ['covid_cases', 'covid_cases_ma7', 'covid_cases_ma30']
            for var in key_vars:
                if var in df.columns:
                    dept_medians = df.groupby('department')[var].median().to_dict()
                    medians[f"{var}_by_dept"] = {str(k): float(v) if not pd.isna(v) else 0.0 
                                                for k, v in dept_medians.items()}
                    logger.info(f"  - {var}_by_dept: {len(dept_medians)} d√©partements")
            
            logger.info(f"‚úÖ M√©dianes calcul√©es: {len(medians)} variables")
            return medians
            
        except Exception as e:
            logger.error(f"‚ùå Erreur calcul m√©dianes: {e}")
            return {}
    
    def calculate_categories(self, df):
        """Calcule les cat√©gories pour les variables cat√©gorielles"""
        logger.info("üìä CALCUL DES CAT√âGORIES")
        logger.info("=" * 50)
        
        try:
            # Identifier les colonnes cat√©gorielles
            categorical_cols = ['department', 'is_weekend', 'is_peak_season']
            
            # Ajouter les colonnes temporelles comme cat√©gories
            if 'month' in df.columns:
                categorical_cols.append('month')
            if 'quarter' in df.columns:
                categorical_cols.append('quarter')
            if 'day_of_week' in df.columns:
                categorical_cols.append('day_of_week')
            
            cats = {}
            
            for col in categorical_cols:
                if col in df.columns:
                    unique_vals = sorted(df[col].unique().tolist())
                    # Convertir en string pour JSON
                    cats[col] = [str(v) for v in unique_vals]
                    logger.info(f"  - {col}: {len(unique_vals)} cat√©gories")
            
            # Ajouter des cat√©gories d√©riv√©es
            if 'month' in df.columns:
                # Saisons
                cats['season'] = ['winter', 'spring', 'summer', 'autumn']
                logger.info(f"  - season: 4 cat√©gories")
            
            # Cat√©gories de temp√©rature (si disponible)
            cats['temp_category'] = ['cold', 'mild', 'hot']
            logger.info(f"  - temp_category: 3 cat√©gories")
            
            logger.info(f"‚úÖ Cat√©gories calcul√©es: {len(cats)} variables")
            return cats
            
        except Exception as e:
            logger.error(f"‚ùå Erreur calcul cat√©gories: {e}")
            return {}
    
    def save_stats(self, medians, cats):
        """Sauvegarde les statistiques"""
        logger.info("üíæ SAUVEGARDE DES STATISTIQUES")
        logger.info("=" * 50)
        
        try:
            # Sauvegarder les m√©dianes
            with open(self.medians_path, 'w', encoding='utf-8') as f:
                json.dump(medians, f, indent=2, ensure_ascii=False)
            logger.info(f"‚úÖ M√©dianes sauvegard√©es: {self.medians_path}")
            
            # Sauvegarder les cat√©gories
            with open(self.cats_path, 'w', encoding='utf-8') as f:
                json.dump(cats, f, indent=2, ensure_ascii=False)
            logger.info(f"‚úÖ Cat√©gories sauvegard√©es: {self.cats_path}")
            
            # Cr√©er un r√©sum√©
            summary = {
                "generated_at": datetime.now().isoformat(),
                "medians_count": len(medians),
                "categories_count": len(cats),
                "medians_variables": list(medians.keys()),
                "categories_variables": list(cats.keys())
            }
            
            summary_path = self.stats_dir / "stats_summary.json"
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üìã R√©sum√© sauvegard√©: {summary_path}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde: {e}")
            return False
    
    def run_fit_stats(self):
        """Lance le calcul des statistiques"""
        logger.info("üöÄ D√âBUT DU CALCUL DES STATISTIQUES")
        logger.info("=" * 60)
        
        # Charger les donn√©es
        df = self.load_timeseries_data()
        if df is None:
            return False
        
        # Calculer les m√©dianes
        medians = self.calculate_medians(df)
        if not medians:
            return False
        
        # Calculer les cat√©gories
        cats = self.calculate_categories(df)
        if not cats:
            return False
        
        # Sauvegarder
        success = self.save_stats(medians, cats)
        
        if success:
            logger.info("‚úÖ CALCUL DES STATISTIQUES TERMIN√â")
            logger.info("=" * 60)
            return True
        else:
            logger.error("‚ùå √âCHEC DU CALCUL DES STATISTIQUES")
            return False

if __name__ == "__main__":
    fitter = StatsFitter()
    fitter.run_fit_stats()
