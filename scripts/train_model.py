#!/usr/bin/env python3
"""
Script d'entraÃ®nement du modÃ¨le Random Forest pour la prÃ©diction grippe
Utilise le dataset fusionnÃ© pour prÃ©dire les urgences J+7, J+14, J+21, J+28
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class GrippePredictor:
    def __init__(self):
        """Initialise le prÃ©dicteur de grippe"""
        self.model = None
        self.feature_columns = None
        self.target_column = 'urgences_grippe'
        self.prediction_horizons = [7, 14, 21, 28]  # J+7, J+14, J+21, J+28
        
    def load_data(self, filepath):
        """Charge le dataset fusionnÃ©"""
        print(f"ğŸ“‚ Chargement du dataset: {filepath}")
        
        df = pd.read_csv(filepath)
        df['date'] = pd.to_datetime(df['date'])
        
        print(f"  âœ… Dataset chargÃ©: {len(df)} enregistrements, {len(df.columns)} colonnes")
        print(f"  ğŸ“… PÃ©riode: {df['date'].min().strftime('%Y-%m-%d')} Ã  {df['date'].max().strftime('%Y-%m-%d')}")
        print(f"  ğŸŒ RÃ©gions: {df['region'].nunique()}")
        
        return df
    
    def prepare_features(self, df):
        """PrÃ©pare les features pour l'entraÃ®nement"""
        print("\nğŸ”§ PrÃ©paration des features...")
        
        # SÃ©lection des features (exclusion des colonnes non pertinentes)
        exclude_cols = ['date', 'region', 'region_code', 'year', 'week_of_year']
        
        # Features de base
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # Suppression des colonnes avec trop de valeurs manquantes
        missing_threshold = 0.5
        valid_cols = []
        for col in feature_cols:
            if col != self.target_column:
                missing_pct = df[col].isna().sum() / len(df)
                if missing_pct < missing_threshold:
                    valid_cols.append(col)
                else:
                    print(f"  âš ï¸ Colonne exclue (trop de NaN): {col} ({missing_pct:.1%})")
        
        self.feature_columns = valid_cols
        print(f"  âœ… {len(self.feature_columns)} features sÃ©lectionnÃ©es")
        
        # Nettoyage des donnÃ©es
        df_clean = df.copy()
        df_clean = df_clean.fillna(0)
        
        # Suppression des lignes avec target manquant
        df_clean = df_clean.dropna(subset=[self.target_column])
        
        print(f"  ğŸ“Š Dataset nettoyÃ©: {len(df_clean)} enregistrements")
        return df_clean
    
    def create_targets(self, df):
        """CrÃ©e les targets pour diffÃ©rents horizons de prÃ©diction"""
        print("\nğŸ¯ CrÃ©ation des targets multi-horizons...")
        
        df_with_targets = df.copy()
        
        for horizon in self.prediction_horizons:
            target_col = f'{self.target_column}_j{horizon}'
            
            # CrÃ©ation du target dÃ©calÃ©
            df_with_targets[target_col] = df_with_targets.groupby('region')[self.target_column].shift(-horizon)
            
            # Statistiques
            valid_targets = df_with_targets[target_col].dropna()
            print(f"  âœ… J+{horizon}: {len(valid_targets)} targets valides")
        
        return df_with_targets
    
    def train_single_horizon(self, df, horizon, test_size=0.2):
        """EntraÃ®ne le modÃ¨le pour un horizon spÃ©cifique"""
        print(f"\nğŸš€ EntraÃ®nement modÃ¨le J+{horizon}...")
        
        target_col = f'{self.target_column}_j{horizon}'
        
        # PrÃ©paration des donnÃ©es
        df_train = df.dropna(subset=[target_col]).copy()
        
        if len(df_train) < 100:
            print(f"  âš ï¸ Pas assez de donnÃ©es pour J+{horizon} ({len(df_train)} Ã©chantillons)")
            return None
        
        X = df_train[self.feature_columns]
        y = df_train[target_col]
        
        print(f"  ğŸ“Š DonnÃ©es d'entraÃ®nement: {len(X)} Ã©chantillons, {len(self.feature_columns)} features")
        
        # Division temporelle
        split_idx = int(len(X) * (1 - test_size))
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
        
        print(f"  ğŸ“ˆ Train: {len(X_train)} Ã©chantillons")
        print(f"  ğŸ“‰ Test: {len(X_test)} Ã©chantillons")
        
        # Configuration du modÃ¨le
        rf = RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        # EntraÃ®nement
        print("  â³ EntraÃ®nement en cours...")
        rf.fit(X_train, y_train)
        
        # PrÃ©dictions
        y_pred_train = rf.predict(X_train)
        y_pred_test = rf.predict(X_test)
        
        # MÃ©triques
        train_mae = mean_absolute_error(y_train, y_pred_train)
        test_mae = mean_absolute_error(y_test, y_pred_test)
        train_r2 = r2_score(y_train, y_pred_train)
        test_r2 = r2_score(y_test, y_pred_test)
        
        print(f"  ğŸ“Š MÃ©triques J+{horizon}:")
        print(f"     Train MAE: {train_mae:.2f}, RÂ²: {train_r2:.3f}")
        print(f"     Test MAE: {test_mae:.2f}, RÂ²: {test_r2:.3f}")
        
        # Importance des features
        feature_importance = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"  ğŸ” Top 5 features importantes:")
        for _, row in feature_importance.head(5).iterrows():
            print(f"     {row['feature']}: {row['importance']:.3f}")
        
        return {
            'model': rf,
            'train_mae': train_mae,
            'test_mae': test_mae,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'feature_importance': feature_importance,
            'horizon': horizon
        }
    
    def train_all_models(self, df):
        """EntraÃ®ne les modÃ¨les pour tous les horizons"""
        print("\nğŸ¯ ENTRAÃNEMENT DES MODÃˆLES MULTI-HORIZONS")
        print("=" * 50)
        
        models = {}
        
        for horizon in self.prediction_horizons:
            model_result = self.train_single_horizon(df, horizon)
            if model_result is not None:
                models[f'j{horizon}'] = model_result
        
        return models
    
    def save_models(self, models, output_dir='../models'):
        """Sauvegarde les modÃ¨les entraÃ®nÃ©s"""
        print(f"\nğŸ’¾ Sauvegarde des modÃ¨les...")
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        saved_models = {}
        
        for model_name, model_data in models.items():
            # Sauvegarde du modÃ¨le
            model_file = f'{output_dir}/rf_grippe_{model_name}_{timestamp}.pkl'
            joblib.dump(model_data['model'], model_file)
            
            # Sauvegarde des mÃ©triques
            metrics_file = f'{output_dir}/metrics_{model_name}_{timestamp}.csv'
            metrics_df = pd.DataFrame({
                'metric': ['train_mae', 'test_mae', 'train_r2', 'test_r2'],
                'value': [model_data['train_mae'], model_data['test_mae'], 
                         model_data['train_r2'], model_data['test_r2']]
            })
            metrics_df.to_csv(metrics_file, index=False)
            
            # Sauvegarde de l'importance des features
            importance_file = f'{output_dir}/importance_{model_name}_{timestamp}.csv'
            model_data['feature_importance'].to_csv(importance_file, index=False)
            
            saved_models[model_name] = {
                'model_file': model_file,
                'metrics_file': metrics_file,
                'importance_file': importance_file,
                'horizon': model_data['horizon']
            }
            
            print(f"  âœ… {model_name}: {model_file}")
        
        # Sauvegarde de la configuration
        config_file = f'{output_dir}/config_{timestamp}.json'
        import json
        config = {
            'feature_columns': self.feature_columns,
            'target_column': self.target_column,
            'prediction_horizons': self.prediction_horizons,
            'models': saved_models,
            'timestamp': timestamp
        }
        
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"  âœ… Configuration: {config_file}")
        return config
    
    def generate_predictions(self, df, models):
        """GÃ©nÃ¨re les prÃ©dictions pour le dataset"""
        print(f"\nğŸ”® GÃ©nÃ©ration des prÃ©dictions...")
        
        df_pred = df.copy()
        
        for model_name, model_data in models.items():
            horizon = model_data['horizon']
            target_col = f'{self.target_column}_j{horizon}'
            pred_col = f'pred_{target_col}'
            
            # PrÃ©dictions
            X = df_pred[self.feature_columns].fillna(0)
            predictions = model_data['model'].predict(X)
            
            df_pred[pred_col] = predictions
            
            print(f"  âœ… {model_name}: {len(predictions)} prÃ©dictions gÃ©nÃ©rÃ©es")
        
        return df_pred

def main():
    """Fonction principale d'entraÃ®nement"""
    print("ğŸš€ ENTRAÃNEMENT DU MODÃˆLE RANDOM FOREST GRIPPE")
    print("=" * 60)
    
    # Initialisation
    predictor = GrippePredictor()
    
    # Recherche du dernier dataset
    processed_dir = '../data/processed'
    
    if not os.path.exists(processed_dir):
        print(f"âŒ Dossier non trouvÃ©: {processed_dir}")
        print("ğŸ’¡ ExÃ©cutez d'abord: python fuse_data.py")
        return
    
    # Liste des fichiers dataset_grippe_*.csv
    dataset_files = [f for f in os.listdir(processed_dir) if f.startswith('dataset_grippe_') and f.endswith('.csv')]
    
    if not dataset_files:
        print(f"âŒ Aucun dataset trouvÃ© dans {processed_dir}")
        print("ğŸ’¡ ExÃ©cutez d'abord: python fuse_data.py")
        return
    
    # Prendre le dernier fichier (tri par nom = tri par timestamp)
    latest_dataset = sorted(dataset_files)[-1]
    dataset_file = os.path.join(processed_dir, latest_dataset)
    
    print(f"ğŸ“‚ Chargement du dataset: {latest_dataset}")
    
    df = predictor.load_data(dataset_file)
    
    # PrÃ©paration des features
    df = predictor.prepare_features(df)
    
    # CrÃ©ation des targets
    df = predictor.create_targets(df)
    
    # EntraÃ®nement des modÃ¨les
    models = predictor.train_all_models(df)
    
    if not models:
        print("âŒ Aucun modÃ¨le entraÃ®nÃ© avec succÃ¨s")
        return
    
    # Sauvegarde des modÃ¨les
    config = predictor.save_models(models)
    
    # GÃ©nÃ©ration des prÃ©dictions
    df_pred = predictor.generate_predictions(df, models)
    
    # Sauvegarde du dataset avec prÃ©dictions
    output_file = f'../data/processed/dataset_with_predictions_{config["timestamp"]}.csv'
    df_pred.to_csv(output_file, index=False)
    print(f"\nğŸ’¾ Dataset avec prÃ©dictions sauvegardÃ©: {output_file}")
    
    # RÃ©sumÃ© final
    print(f"\nâœ… ENTRAÃNEMENT TERMINÃ‰")
    print(f"ğŸ“Š {len(models)} modÃ¨les entraÃ®nÃ©s")
    print(f"ğŸ¯ Horizons: {list(models.keys())}")
    print(f"ğŸ“ ModÃ¨les sauvegardÃ©s dans: models/")

if __name__ == "__main__":
    main()
