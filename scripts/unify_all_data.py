#!/usr/bin/env python3
"""
Script pour unifier toutes les donn√©es par chunks et cr√©er le dataset final complet
"""

import pandas as pd
import json
from pathlib import Path
import logging
from datetime import datetime

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataUnifier:
    def __init__(self):
        self.base_dir = Path("data")
        self.processed_dir = self.base_dir / "processed"
        self.external_dir = self.processed_dir / "external_data"
        self.output_dir = self.processed_dir
        self.output_dir.mkdir(exist_ok=True)
    
    def load_vaccination_data(self):
        """Charge les donn√©es de vaccination"""
        logger.info("üìä CHARGEMENT DES DONN√âES DE VACCINATION")
        logger.info("=" * 40)
        
        try:
            vaccination_file = self.processed_dir / "lumen_final_dataset.parquet"
            df = pd.read_parquet(vaccination_file)
            
            logger.info(f"‚úÖ Vaccination: {df.shape}")
            logger.info(f"   Colonnes: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur vaccination: {e}")
            return None
    
    def load_demographic_data(self):
        """Charge les donn√©es d√©mographiques"""
        logger.info("üë• CHARGEMENT DES DONN√âES D√âMOGRAPHIQUES")
        logger.info("=" * 40)
        
        try:
            demo_file = self.external_dir / "demographic_data.parquet"
            df = pd.read_parquet(demo_file)
            
            logger.info(f"‚úÖ D√©mographie: {df.shape}")
            logger.info(f"   Colonnes: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©mographie: {e}")
            return None
    
    def load_behavioral_data(self):
        """Charge les donn√©es comportementales"""
        logger.info("üß† CHARGEMENT DES DONN√âES COMPORTEMENTALES")
        logger.info("=" * 40)
        
        try:
            behavior_file = self.external_dir / "behavioral_data.parquet"
            df = pd.read_parquet(behavior_file)
            
            logger.info(f"‚úÖ Comportement: {df.shape}")
            logger.info(f"   Colonnes: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur comportement: {e}")
            return None
    
    def load_weather_data(self):
        """Charge les donn√©es m√©t√©o par chunks"""
        logger.info("üå¶Ô∏è CHARGEMENT DES DONN√âES M√âT√âO")
        logger.info("=" * 40)
        
        try:
            weather_file = self.external_dir / "weather_data_gouv.parquet"
            
            # Lire par chunks pour √©viter les probl√®mes de m√©moire
            chunk_size = 50000  # 50k lignes par chunk
            weather_chunks = []
            
            logger.info(f"üìÅ Lecture par chunks de {chunk_size} lignes...")
            
            for i, chunk in enumerate(pd.read_parquet(weather_file, chunksize=chunk_size)):
                logger.info(f"   Chunk {i+1}: {len(chunk)} lignes")
                
                # Nettoyage basique du chunk
                chunk = self.clean_weather_chunk(chunk)
                
                if not chunk.empty:
                    weather_chunks.append(chunk)
            
            if weather_chunks:
                # Unifier les chunks m√©t√©o
                weather_df = pd.concat(weather_chunks, ignore_index=True, sort=False)
                logger.info(f"‚úÖ M√©t√©o unifi√©e: {weather_df.shape}")
                return weather_df
            else:
                logger.warning("‚ö†Ô∏è Aucun chunk m√©t√©o valide")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erreur m√©t√©o: {e}")
            return None
    
    def clean_weather_chunk(self, chunk):
        """Nettoie un chunk de donn√©es m√©t√©o"""
        try:
            # Garder seulement les colonnes utiles
            useful_cols = []
            for col in chunk.columns:
                if any(keyword in col.lower() for keyword in ['temp', 'temperature', 'date', 'region', 'ville', 'city']):
                    useful_cols.append(col)
            
            if useful_cols:
                chunk = chunk[useful_cols]
            
            # Supprimer les lignes avec trop de NaN
            chunk = chunk.dropna(thresh=len(chunk.columns) * 0.5)
            
            return chunk
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur nettoyage chunk m√©t√©o: {e}")
            return pd.DataFrame()
    
    def create_features(self, vaccination_df, demographic_df, behavioral_df, weather_df):
        """Cr√©e des features engineering pour l'IA"""
        logger.info("üîß CR√âATION DES FEATURES")
        logger.info("=" * 40)
        
        try:
            # Commencer avec les donn√©es de vaccination
            final_df = vaccination_df.copy()
            
            # Ajouter les features d√©mographiques
            if demographic_df is not None:
                logger.info("üë• Ajout des features d√©mographiques...")
                
                # Mapper les r√©gions
                region_mapping = {}
                for _, row in demographic_df.iterrows():
                    region_mapping[row['region']] = {
                        'population': row['population'],
                        'density': row['density']
                    }
                
                # Ajouter les features d√©mographiques
                final_df['population'] = final_df['region'].map(lambda x: region_mapping.get(x, {}).get('population', 0))
                final_df['density'] = final_df['region'].map(lambda x: region_mapping.get(x, {}).get('density', 0))
            
            # Ajouter les features comportementales
            if behavioral_df is not None:
                logger.info("üß† Ajout des features comportementales...")
                
                # Calculer un score comportemental bas√© sur la longueur des textes
                total_text_length = behavioral_df['text_length'].sum()
                final_df['behavioral_score'] = total_text_length / len(behavioral_df) if len(behavioral_df) > 0 else 0
            
            # Ajouter les features m√©t√©o (√©chantillonnage)
            if weather_df is not None:
                logger.info("üå¶Ô∏è Ajout des features m√©t√©o...")
                
                # Prendre un √©chantillon des donn√©es m√©t√©o pour √©viter la surcharge
                weather_sample = weather_df.sample(n=min(1000, len(weather_df)), random_state=42)
                
                # Ajouter des features m√©t√©o moyennes
                if 'temp' in str(weather_sample.columns).lower():
                    temp_cols = [col for col in weather_sample.columns if 'temp' in col.lower()]
                    if temp_cols:
                        final_df['avg_temperature'] = weather_sample[temp_cols[0]].mean() if len(temp_cols) > 0 else 20
                else:
                    final_df['avg_temperature'] = 20  # Temp√©rature par d√©faut
            
            # Cr√©er des features temporelles
            logger.info("üìÖ Cr√©ation des features temporelles...")
            
            # Features saisonni√®res
            final_df['season'] = 'unknown'
            if 'campagne' in final_df.columns:
                final_df['season'] = final_df['campagne'].apply(self.get_season)
            
            # Features g√©ographiques
            if 'region' in final_df.columns:
                final_df['region_code'] = final_df['region'].str.extract(r'(\d+)').astype(float)
            
            # Features de groupe d'√¢ge
            if 'groupe' in final_df.columns:
                final_df['age_group'] = final_df['groupe'].apply(self.get_age_group_code)
            
            logger.info(f"‚úÖ Features cr√©√©es: {final_df.shape}")
            return final_df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation features: {e}")
            return vaccination_df
    
    def get_season(self, campagne):
        """D√©termine la saison bas√©e sur la campagne"""
        if pd.isna(campagne):
            return 'unknown'
        
        campagne_str = str(campagne)
        if '2021' in campagne_str or '2022' in campagne_str:
            return 'winter'
        elif '2023' in campagne_str:
            return 'spring'
        elif '2024' in campagne_str:
            return 'summer'
        elif '2025' in campagne_str:
            return 'autumn'
        else:
            return 'unknown'
    
    def get_age_group_code(self, groupe):
        """Convertit le groupe d'√¢ge en code num√©rique"""
        if pd.isna(groupe):
            return 0
        
        groupe_str = str(groupe).lower()
        if 'moins de 65' in groupe_str:
            return 1
        elif '65 ans' in groupe_str:
            return 2
        else:
            return 0
    
    def save_final_dataset(self, final_df):
        """Sauvegarde le dataset final"""
        logger.info("üíæ SAUVEGARDE DU DATASET FINAL")
        logger.info("=" * 40)
        
        try:
            # Sauvegarder le dataset complet
            final_file = self.output_dir / "lumen_complete_dataset.parquet"
            final_df.to_parquet(final_file, index=False)
            logger.info(f"‚úÖ Dataset complet sauvegard√©: {final_file}")
            
            # Cr√©er un r√©sum√©
            summary = {
                "total_rows": len(final_df),
                "total_columns": len(final_df.columns),
                "columns": list(final_df.columns),
                "target_variable": "valeur",
                "features": {
                    "temporal": ["season", "campagne"],
                    "geographic": ["region", "region_code", "population", "density"],
                    "demographic": ["age_group", "groupe"],
                    "behavioral": ["behavioral_score"],
                    "weather": ["avg_temperature"],
                    "target": ["valeur"]
                },
                "creation_date": datetime.now().isoformat()
            }
            
            summary_file = self.output_dir / "dataset_summary.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üìã R√©sum√© sauvegard√©: {summary_file}")
            
            # Afficher les statistiques
            self.print_dataset_stats(final_df)
            
            return final_df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde: {e}")
            return None
    
    def print_dataset_stats(self, df):
        """Affiche les statistiques du dataset final"""
        logger.info("üìä STATISTIQUES DU DATASET FINAL")
        logger.info("=" * 50)
        logger.info(f"üìä Shape: {df.shape}")
        logger.info(f"üìÖ Colonnes: {list(df.columns)}")
        
        # Statistiques par source
        if 'source_file' in df.columns:
            logger.info("üìÅ Par source:")
            for source in df['source_file'].unique():
                count = len(df[df['source_file'] == source])
                logger.info(f"  - {source}: {count:,} lignes")
        
        # Statistiques par r√©gion
        if 'region' in df.columns:
            logger.info("üó∫Ô∏è Par r√©gion:")
            for region in df['region'].unique()[:5]:  # Top 5
                count = len(df[df['region'] == region])
                logger.info(f"  - {region}: {count:,} lignes")
        
        # Statistiques de la target variable
        if 'valeur' in df.columns:
            logger.info("üéØ Target variable (valeur):")
            logger.info(f"  - Moyenne: {df['valeur'].mean():.2f}")
            logger.info(f"  - M√©diane: {df['valeur'].median():.2f}")
            logger.info(f"  - Min: {df['valeur'].min():.2f}")
            logger.info(f"  - Max: {df['valeur'].max():.2f}")
        
        logger.info("=" * 50)
    
    def run_unification(self):
        """Lance l'unification compl√®te des donn√©es"""
        logger.info("üöÄ D√âBUT DE L'UNIFICATION DES DONN√âES")
        logger.info("=" * 60)
        
        # Charger toutes les donn√©es
        vaccination_df = self.load_vaccination_data()
        demographic_df = self.load_demographic_data()
        behavioral_df = self.load_behavioral_data()
        weather_df = self.load_weather_data()
        
        if vaccination_df is None:
            logger.error("‚ùå Impossible de continuer sans donn√©es de vaccination")
            return None
        
        # Cr√©er les features
        final_df = self.create_features(vaccination_df, demographic_df, behavioral_df, weather_df)
        
        # Sauvegarder le dataset final
        result = self.save_final_dataset(final_df)
        
        logger.info("‚úÖ UNIFICATION TERMIN√âE")
        logger.info("=" * 60)
        
        return result

if __name__ == "__main__":
    unifier = DataUnifier()
    unifier.run_unification()
