#!/usr/bin/env python3
"""
ğŸ¤– ENTRAÃNEMENT RANDOM FOREST - LUMEN
SystÃ¨me ML complet : entraÃ®nement, Ã©valuation et prÃ©dictions
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, classification_report
import joblib
import os
import json
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class LumenMLTrainer:
    def __init__(self):
        self.dataset_path = "data/processed/clean_dataset.csv"
        self.artifacts_dir = "ml/artefacts"
        self.predictions_path = "data/processed/predictions.csv"
        
        # CrÃ©er les dossiers
        os.makedirs(self.artifacts_dir, exist_ok=True)
        os.makedirs("data/processed", exist_ok=True)
        
        self.df = None
        self.features = []
        self.target = None
        self.rf_model = None
        self.metrics = {}

    def load_dataset(self):
        """Charge le dataset fusionnÃ©"""
        print("ğŸ“Š CHARGEMENT DU DATASET FUSIONNÃ‰")
        print("=" * 40)
        
        try:
            self.df = pd.read_csv(self.dataset_path)
            print(f"âœ… Dataset chargÃ©: {self.df.shape}")
            print(f"ğŸ“Š Colonnes disponibles: {list(self.df.columns)}")
            return True
        except Exception as e:
            print(f"âŒ Erreur chargement dataset: {e}")
            return False

    def prepare_features_and_target(self):
        """PrÃ©pare les features et la variable cible"""
        print("\nğŸ¯ PRÃ‰PARATION DES FEATURES ET CIBLE")
        print("=" * 45)
        
        # Analyser les colonnes disponibles
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        print(f"ğŸ“Š Colonnes numÃ©riques disponibles: {len(numeric_cols)}")
        
        # Identifier la variable cible (nb_passages semble Ãªtre notre cible principale)
        if 'nb_passages' in self.df.columns:
            self.target = 'nb_passages'
            print(f"ğŸ¯ Variable cible identifiÃ©e: {self.target}")
        else:
            # Prendre la premiÃ¨re colonne numÃ©rique comme cible
            self.target = numeric_cols[0]
            print(f"ğŸ¯ Variable cible par dÃ©faut: {self.target}")
        
        # SÃ©lectionner les features (exclure la cible et les colonnes non pertinentes)
        exclude_cols = [
            self.target, '_source_file', 'date', 'date_de_passage', 
            'geo_point_2d', 'geo_point', 'geometry'
        ]
        
        self.features = [col for col in numeric_cols if col not in exclude_cols]
        
        print(f"ğŸ”§ Features sÃ©lectionnÃ©es ({len(self.features)}):")
        for i, feature in enumerate(self.features, 1):
            print(f"   {i:2d}. {feature}")
        
        return len(self.features) > 0

    def train_regression_model(self):
        """EntraÃ®ne le modÃ¨le de rÃ©gression Random Forest"""
        print("\nğŸ¤– ENTRAÃNEMENT MODÃˆLE DE RÃ‰GRESSION")
        print("=" * 45)
        
        # PrÃ©parer les donnÃ©es
        X = self.df[self.features].fillna(0)  # Remplacer NaN par 0
        y = self.df[self.target].fillna(0)
        
        print(f"ğŸ“Š Features shape: {X.shape}")
        print(f"ğŸ“Š Target shape: {y.shape}")
        print(f"ğŸ“Š Target stats: min={y.min():.2f}, max={y.max():.2f}, mean={y.mean():.2f}")
        
        # Split train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        print(f"ğŸ“Š Train: {X_train.shape[0]} Ã©chantillons")
        print(f"ğŸ“Š Test: {X_test.shape[0]} Ã©chantillons")
        
        # EntraÃ®nement Random Forest
        print("ğŸ”„ EntraÃ®nement en cours...")
        self.rf_model = RandomForestRegressor(
            n_estimators=300,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        self.rf_model.fit(X_train, y_train)
        
        # PrÃ©dictions et Ã©valuation
        y_pred = self.rf_model.predict(X_test)
        
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        print(f"âœ… MAE: {mae:.3f}")
        print(f"âœ… RÂ²: {r2:.3f}")
        
        # Sauvegarder les mÃ©triques
        self.metrics = {
            "model_type": "RandomForestRegressor",
            "target": self.target,
            "features_count": len(self.features),
            "train_samples": len(X_train),
            "test_samples": len(X_test),
            "MAE": float(mae),
            "R2": float(r2),
            "timestamp": datetime.now().isoformat()
        }
        
        return True

    def analyze_feature_importance(self):
        """Analyse l'importance des features"""
        print("\nğŸ“Š ANALYSE DE L'IMPORTANCE DES FEATURES")
        print("=" * 45)
        
        # Calculer l'importance des features
        importances = pd.Series(
            self.rf_model.feature_importances_, 
            index=self.features
        ).sort_values(ascending=True)
        
        print("ğŸ† TOP 10 FEATURES LES PLUS IMPORTANTES:")
        for i, (feature, importance) in enumerate(importances.tail(10).items(), 1):
            print(f"   {i:2d}. {feature:<25} {importance:.4f}")
        
        # CrÃ©er le graphique d'importance
        plt.figure(figsize=(10, 8))
        importances.plot(kind="barh", color="teal")
        plt.title("Importance des variables - Random Forest", fontsize=14, fontweight='bold')
        plt.xlabel("Importance", fontsize=12)
        plt.ylabel("Variables", fontsize=12)
        plt.tight_layout()
        
        # Sauvegarder le graphique
        importance_plot_path = os.path.join(self.artifacts_dir, "feature_importance.png")
        plt.savefig(importance_plot_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š Graphique sauvegardÃ©: {importance_plot_path}")
        
        # Sauvegarder les importances en CSV
        importance_df = pd.DataFrame({
            'feature': importances.index,
            'importance': importances.values
        })
        importance_csv_path = os.path.join(self.artifacts_dir, "feature_importance.csv")
        importance_df.to_csv(importance_csv_path, index=False)
        print(f"ğŸ“Š CSV sauvegardÃ©: {importance_csv_path}")
        
        plt.close()

    def generate_predictions(self):
        """GÃ©nÃ¨re les prÃ©dictions pour tout le dataset"""
        print("\nğŸ”® GÃ‰NÃ‰RATION DES PRÃ‰DICTIONS")
        print("=" * 35)
        
        # PrÃ©parer les features
        X = self.df[self.features].fillna(0)
        
        # GÃ©nÃ©rer les prÃ©dictions
        print("ğŸ”„ GÃ©nÃ©ration des prÃ©dictions...")
        predictions = self.rf_model.predict(X)
        
        # Ajouter les prÃ©dictions au dataset
        self.df["pred_" + self.target] = predictions
        self.df["ecart"] = self.df[self.target] - self.df["pred_" + self.target]
        
        # Calculer des mÃ©triques supplÃ©mentaires
        self.df["ecart_absolu"] = abs(self.df["ecart"])
        self.df["ecart_pct"] = (self.df["ecart"] / (self.df[self.target] + 1e-8)) * 100
        
        print(f"âœ… PrÃ©dictions gÃ©nÃ©rÃ©es pour {len(predictions)} Ã©chantillons")
        print(f"ğŸ“Š Ã‰cart moyen: {self.df['ecart'].mean():.3f}")
        print(f"ğŸ“Š Ã‰cart absolu moyen: {self.df['ecart_absolu'].mean():.3f}")
        print(f"ğŸ“Š Ã‰cart % moyen: {self.df['ecart_pct'].mean():.1f}%")
        
        return True

    def save_model_and_artifacts(self):
        """Sauvegarde le modÃ¨le et les artefacts"""
        print("\nğŸ’¾ SAUVEGARDE DU MODÃˆLE ET ARTEFACTS")
        print("=" * 45)
        
        # Sauvegarder le modÃ¨le
        model_path = os.path.join(self.artifacts_dir, "random_forest.pkl")
        joblib.dump(self.rf_model, model_path)
        print(f"âœ… ModÃ¨le sauvegardÃ©: {model_path}")
        
        # Sauvegarder les mÃ©triques
        metrics_path = os.path.join(self.artifacts_dir, "metrics.json")
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, indent=2, ensure_ascii=False)
        print(f"âœ… MÃ©triques sauvegardÃ©es: {metrics_path}")
        
        # Sauvegarder les prÃ©dictions
        self.df.to_csv(self.predictions_path, index=False)
        print(f"âœ… PrÃ©dictions sauvegardÃ©es: {self.predictions_path}")
        
        # CrÃ©er un rapport de performance
        self.create_performance_report()

    def create_performance_report(self):
        """CrÃ©e un rapport de performance dÃ©taillÃ©"""
        print("\nğŸ“‹ CRÃ‰ATION DU RAPPORT DE PERFORMANCE")
        print("=" * 45)
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "model_info": {
                "type": "RandomForestRegressor",
                "target_variable": self.target,
                "features_used": self.features,
                "features_count": len(self.features)
            },
            "performance": self.metrics,
            "predictions_info": {
                "total_predictions": len(self.df),
                "mean_prediction": float(self.df["pred_" + self.target].mean()),
                "mean_actual": float(self.df[self.target].mean()),
                "mean_error": float(self.df["ecart"].mean()),
                "mean_absolute_error": float(self.df["ecart_absolu"].mean()),
                "mean_percentage_error": float(self.df["ecart_pct"].mean())
            }
        }
        
        # Sauvegarder le rapport
        report_path = os.path.join(self.artifacts_dir, "performance_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ Rapport de performance: {report_path}")
        
        # CrÃ©er un rÃ©sumÃ© lisible
        summary_path = os.path.join(self.artifacts_dir, "TRAINING_SUMMARY.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("ğŸ¤– RAPPORT D'ENTRAÃNEMENT - LUMEN ML\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ğŸ“… Date: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n")
            f.write(f"ğŸ¯ Variable cible: {self.target}\n")
            f.write(f"ğŸ”§ Features utilisÃ©es: {len(self.features)}\n")
            f.write(f"ğŸ“Š Ã‰chantillons d'entraÃ®nement: {self.metrics['train_samples']:,}\n")
            f.write(f"ğŸ“Š Ã‰chantillons de test: {self.metrics['test_samples']:,}\n\n")
            
            f.write("ğŸ“ˆ PERFORMANCE:\n")
            f.write("-" * 15 + "\n")
            f.write(f"â€¢ MAE (Erreur Absolue Moyenne): {self.metrics['MAE']:.3f}\n")
            f.write(f"â€¢ RÂ² (Coefficient de dÃ©termination): {self.metrics['R2']:.3f}\n\n")
            
            f.write("ğŸ”® PRÃ‰DICTIONS:\n")
            f.write("-" * 12 + "\n")
            f.write(f"â€¢ Total prÃ©dictions: {len(self.df):,}\n")
            f.write(f"â€¢ Valeur moyenne prÃ©dite: {self.df['pred_' + self.target].mean():.2f}\n")
            f.write(f"â€¢ Valeur moyenne rÃ©elle: {self.df[self.target].mean():.2f}\n")
            f.write(f"â€¢ Ã‰cart moyen: {self.df['ecart'].mean():.2f}\n")
            f.write(f"â€¢ Ã‰cart absolu moyen: {self.df['ecart_absolu'].mean():.2f}\n")
            f.write(f"â€¢ Erreur % moyenne: {self.df['ecart_pct'].mean():.1f}%\n\n")
            
            f.write("ğŸ† TOP 5 FEATURES IMPORTANTES:\n")
            f.write("-" * 30 + "\n")
            importances = pd.Series(self.rf_model.feature_importances_, index=self.features)
            top_features = importances.sort_values(ascending=False).head(5)
            for i, (feature, importance) in enumerate(top_features.items(), 1):
                f.write(f"{i}. {feature}: {importance:.4f}\n")
        
        print(f"ğŸ“„ RÃ©sumÃ©: {summary_path}")

    def run_training_pipeline(self):
        """Lance le pipeline d'entraÃ®nement complet"""
        print("ğŸ¤– ENTRAÃNEMENT RANDOM FOREST - LUMEN")
        print("=" * 50)
        print("ğŸ¯ SystÃ¨me ML complet : entraÃ®nement, Ã©valuation, prÃ©dictions")
        print(f"â° DÃ©but: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        
        # 1. Charger le dataset
        if not self.load_dataset():
            return False
        
        # 2. PrÃ©parer features et cible
        if not self.prepare_features_and_target():
            print("âŒ Impossible de prÃ©parer les features")
            return False
        
        # 3. EntraÃ®ner le modÃ¨le
        if not self.train_regression_model():
            print("âŒ Ã‰chec de l'entraÃ®nement")
            return False
        
        # 4. Analyser l'importance des features
        self.analyze_feature_importance()
        
        # 5. GÃ©nÃ©rer les prÃ©dictions
        if not self.generate_predictions():
            print("âŒ Ã‰chec de gÃ©nÃ©ration des prÃ©dictions")
            return False
        
        # 6. Sauvegarder tout
        self.save_model_and_artifacts()
        
        # RÃ©sumÃ© final
        print("\nğŸ‰ ENTRAÃNEMENT TERMINÃ‰ AVEC SUCCÃˆS")
        print("=" * 40)
        print(f"ğŸ¯ Variable cible: {self.target}")
        print(f"ğŸ”§ Features: {len(self.features)}")
        print(f"ğŸ“Š Performance RÂ²: {self.metrics['R2']:.3f}")
        print(f"ğŸ“Š Performance MAE: {self.metrics['MAE']:.3f}")
        print(f"ğŸ”® PrÃ©dictions: {len(self.df):,} Ã©chantillons")
        print(f"ğŸ’¾ ModÃ¨le: ml/artefacts/random_forest.pkl")
        print(f"ğŸ“Š PrÃ©dictions: data/processed/predictions.csv")
        print(f"â° Fin: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")

if __name__ == "__main__":
    trainer = LumenMLTrainer()
    trainer.run_training_pipeline()
