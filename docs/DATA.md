# üìä Donn√©es - Collecte et traitement

Ce document explique o√π se trouvent les donn√©es, comment elles sont collect√©es, trait√©es et utilis√©es dans LUMEN.

## üìÅ Structure des donn√©es

```
data/
‚îú‚îÄ‚îÄ spf/                    # Sant√© Publique France
‚îÇ   ‚îú‚îÄ‚îÄ sentinelles_*.csv
‚îÇ   ‚îú‚îÄ‚îÄ urgences_*.csv
‚îÇ   ‚îî‚îÄ‚îÄ vaccination_*.csv
‚îú‚îÄ‚îÄ insee/                  # Donn√©es d√©mographiques
‚îÇ   ‚îî‚îÄ‚îÄ insee_*.csv
‚îú‚îÄ‚îÄ meteo/                  # Donn√©es m√©t√©orologiques
‚îÇ   ‚îî‚îÄ‚îÄ meteo_*.csv
‚îú‚îÄ‚îÄ wikipedia/              # Pages vues Wikipedia
‚îÇ   ‚îî‚îÄ‚îÄ wikipedia_*.csv
‚îú‚îÄ‚îÄ google_trends/          # Tendances de recherche
‚îÇ   ‚îî‚îÄ‚îÄ trends_*.csv
‚îú‚îÄ‚îÄ processed/              # Donn√©es fusionn√©es
‚îÇ   ‚îî‚îÄ‚îÄ dataset_with_alerts_*.csv
‚îú‚îÄ‚îÄ alerts/                 # Alertes g√©n√©r√©es
‚îÇ   ‚îú‚îÄ‚îÄ alertes_*.csv
‚îÇ   ‚îî‚îÄ‚îÄ protocoles_*.csv
‚îî‚îÄ‚îÄ collection_config.json  # Configuration de collecte
```

**Note :** Tous les fichiers CSV utilisent un timestamp `YYYYMMDD_HHMMSS` pour versionning.

## üîÑ Pipeline de donn√©es

### Vue d'ensemble

```
1. COLLECTE              2. FUSION              3. ALERTES              4. VISUALISATION
   ‚Üì                         ‚Üì                      ‚Üì                       ‚Üì
generate_demo_data.py ‚Üí dataset_with_alerts ‚Üí alertes + protocoles ‚Üí app_complete.py
   ‚Üì                         ‚Üì                      ‚Üì                       ‚Üì
data/[sources]/          data/processed/       data/alerts/            Interface web
```

## 1Ô∏è‚É£ Collecte des donn√©es

### Script : `scripts/generate_demo_data.py`

**Ce qu'il fait :**
- G√©n√®re des donn√©es de d√©monstration bas√©es sur des patterns r√©alistes
- Simule les variations saisonni√®res (pic hivernal)
- Cr√©e des donn√©es coh√©rentes pour les 13 r√©gions fran√ßaises

**Commande :**
```bash
venv/bin/python scripts/generate_demo_data.py
```

**Sortie :**
```
‚úÖ Donn√©es de d√©monstration g√©n√©r√©es :
   üìä Dataset: data/processed/dataset_with_alerts_20251021_104801.csv
   üö® Alertes: data/alerts/alertes_20251021_104801.csv
   üìã Protocoles: data/alerts/protocoles_20251021_104801.csv
   ‚öôÔ∏è  Configuration: models/config_20251021_104801.json
```

### Donn√©es g√©n√©r√©es

#### Structure du dataset principal

```csv
date,region,urgences_grippe,vaccination_2024,ias_syndrome_grippal,population_totale,pct_65_plus,alert_score,flurisk,pred_urgences_grippe_j7,pred_urgences_grippe_j14,pred_urgences_grippe_j21,pred_urgences_grippe_j28
2024-10-21,√éle-de-France,125,67.5,2.3,12000000,18.5,42.3,42.3,138,150,163,175
```

**Colonnes principales :**
- `date` : Date de l'observation (YYYY-MM-DD)
- `region` : Nom de la r√©gion fran√ßaise
- `urgences_grippe` : Nombre de passages aux urgences pour grippe
- `vaccination_2024` : Taux de vaccination (0-100%)
- `ias_syndrome_grippal` : Indicateur d'activit√© syndromique
- `population_totale` : Population de la r√©gion
- `pct_65_plus` : Pourcentage de population 65+ ans
- `alert_score` : Score d'alerte calcul√© (0-100)
- `flurisk` : Alias de alert_score
- `pred_urgences_grippe_j7` : Pr√©diction √† 7 jours
- `pred_urgences_grippe_j14` : Pr√©diction √† 14 jours
- `pred_urgences_grippe_j21` : Pr√©diction √† 21 jours
- `pred_urgences_grippe_j28` : Pr√©diction √† 28 jours

#### Structure des alertes

```csv
region,level,alert_score,action,timeline,urgences_actuelles,vaccination_rate
√éle-de-France,√âLEV√â,72.5,Pr√©parer campagne,1-2 semaines,85,52.3
```

**Colonnes :**
- `region` : R√©gion concern√©e
- `level` : Niveau d'alerte (CRITIQUE, √âLEV√â, MOD√âR√â, FAIBLE)
- `alert_score` : Score num√©rique (0-100)
- `action` : Action recommand√©e
- `timeline` : D√©lai de mise en ≈ìuvre
- `urgences_actuelles` : Nombre d'urgences actuelles
- `vaccination_rate` : Taux de vaccination actuel

#### Structure des protocoles

```csv
region,protocol,estimated_cost,expected_roi,timeline
√éle-de-France,Campagne de vaccination cibl√©e - √éle-de-France,125000,3.5,1-2 semaines
```

**Colonnes :**
- `region` : R√©gion concern√©e
- `protocol` : Description du protocole
- `estimated_cost` : Co√ªt estim√© (‚Ç¨)
- `expected_roi` : Retour sur investissement attendu
- `timeline` : D√©lai de mise en ≈ìuvre

## 2Ô∏è‚É£ Fusion des donn√©es (pipeline complet)

### Architecture compl√®te (si utilisation de vraies donn√©es)

Pour utiliser de vraies donn√©es au lieu des donn√©es de d√©monstration :

#### √âtape 1 : Collecte

**Script :** `scripts/collect_real_data_fixed.py`

```bash
venv/bin/python scripts/collect_real_data_fixed.py
```

**Actions :**
- Collecte ou simule les donn√©es SPF (sentinelles, urgences, vaccination)
- Collecte les donn√©es INSEE (population)
- Collecte les donn√©es m√©t√©o (Open-Meteo API)
- G√©n√®re `collection_config.json` avec la liste des fichiers

**Fichiers cr√©√©s :**
```
data/spf/sentinelles_real_YYYYMMDD_HHMMSS.csv
data/spf/urgences_real_YYYYMMDD_HHMMSS.csv
data/spf/vaccination_real_YYYYMMDD_HHMMSS.csv
data/insee/insee_real_YYYYMMDD_HHMMSS.csv
data/meteo/meteo_real_YYYYMMDD_HHMMSS.csv
```

#### √âtape 2 : Fusion

**Script :** `scripts/fuse_data.py`

```bash
venv/bin/python scripts/fuse_data.py
```

**Actions :**
- Lit le dernier fichier de chaque source
- Fusionne sur les cl√©s `region` + `date`
- Calcule des features d√©riv√©es
- Sauvegarde le dataset unifi√©

**Fichier cr√©√© :**
```
data/processed/dataset_fused_YYYYMMDD_HHMMSS.csv
```

#### √âtape 3 : G√©n√©ration d'alertes

**Script :** `scripts/create_alert_system.py`

```bash
venv/bin/python scripts/create_alert_system.py
```

**Actions :**
- Lit le dataset fusionn√©
- Calcule les scores d'alerte par r√©gion
- G√©n√®re les protocoles d'action recommand√©s
- Sauvegarde alertes et protocoles

**Fichiers cr√©√©s :**
```
data/alerts/alertes_YYYYMMDD_HHMMSS.csv
data/alerts/protocoles_YYYYMMDD_HHMMSS.csv
data/processed/dataset_with_alerts_YYYYMMDD_HHMMSS.csv
```

## 3Ô∏è‚É£ Utilisation par l'application

### Chargement des donn√©es

**Fichier :** `app_complete.py`

```python
# L'application charge toujours le fichier le plus r√©cent
alert_files = [f for f in os.listdir('data/processed')
               if f.startswith('dataset_with_alerts_')]
if alert_files:
    latest_file = sorted(alert_files)[-1]
    self.data = pd.read_csv(f'data/processed/{latest_file}')
```

**Pattern de nommage :**
- Le tri lexicographique des fichiers avec timestamp donne automatiquement le plus r√©cent
- Format : `dataset_with_alerts_20251021_104801.csv`
- Tri : `20251021_104801` > `20251020_153000` ‚úÖ

### Mise en cache

L'application utilise le cache Streamlit pour optimiser les performances :

```python
@st.cache_data
def load_data():
    return pd.read_csv('data/processed/dataset_with_alerts_latest.csv')
```

## üîß Configuration de collecte

### Fichier : `data/collection_config.json`

```json
{
  "collection_date": "2025-10-21T10:31:50.763370",
  "files": [
    "data/spf/sentinelles_real_20251021_103150.csv",
    "data/spf/urgences_real_20251021_103150.csv",
    "data/spf/vaccination_real_20251021_103150.csv",
    "data/insee/insee_real_20251021_103150.csv",
    "data/meteo/meteo_real_20251021_103150.csv"
  ],
  "sources": [
    "Sant√© Publique France",
    "INSEE",
    "M√©t√©o France"
  ],
  "data_type": "real_based",
  "description": "Donn√©es g√©n√©r√©es bas√©es sur les patterns r√©els des sources officielles"
}
```

**Utilit√© :**
- Tra√ßabilit√© des donn√©es collect√©es
- Liste des fichiers sources utilis√©s
- Date de derni√®re collecte
- Type de donn√©es (demo, real, real_based)

## üìà Calcul du score d'alerte

### Algorithme

```python
alert_score = min(100, max(0,
    (100 - vaccination) * 0.4 +      # 40% : Faible vaccination
    (urgences / 100) * 0.3 +          # 30% : Urgences √©lev√©es
    (ias * 10) * 0.3                  # 30% : IAS √©lev√©
))
```

**Pond√©ration :**
- Vaccination : 40% (inversement proportionnel)
- Urgences : 30% (directement proportionnel)
- IAS : 30% (directement proportionnel)

### Niveaux d'alerte

| Score | Niveau | Couleur | Action |
|-------|--------|---------|--------|
| 0-40 | FAIBLE | üü¢ Vert | Surveillance normale |
| 40-60 | MOD√âR√â | üü° Jaune | Pr√©paration |
| 60-80 | √âLEV√â | üü† Orange | Campagne cibl√©e |
| 80-100 | CRITIQUE | üî¥ Rouge | Action imm√©diate |

## üîÑ Mise √† jour des donn√©es

### Manuelle

```bash
# R√©g√©n√©rer les donn√©es de d√©monstration
venv/bin/python scripts/generate_demo_data.py

# Relancer l'application (elle chargera automatiquement les nouvelles donn√©es)
venv/bin/python launch_app.py
```

### Automatique (avec cron)

**Linux/Mac :**
```bash
# √âditer la crontab
crontab -e

# Ajouter (tous les lundis √† 8h)
0 8 * * 1 cd /path/to/t-hack-700 && venv/bin/python scripts/generate_demo_data.py
```

**Windows (Planificateur de t√¢ches) :**
1. Ouvrir le Planificateur de t√¢ches
2. Cr√©er une t√¢che basique
3. D√©clencheur : Hebdomadaire, lundi 8h
4. Action : Lancer `C:\path\to\venv\Scripts\python.exe scripts\generate_demo_data.py`

## üóÑÔ∏è Nettoyage des anciennes donn√©es

### Script de nettoyage

Cr√©er `scripts/cleanup_old_data.py` :

```python
import os
from datetime import datetime, timedelta

def cleanup_old_files(directory, days=30):
    """Supprime les fichiers plus vieux que X jours"""
    cutoff = datetime.now() - timedelta(days=days)

    for filename in os.listdir(directory):
        filepath = os.path.join(directory, filename)
        if os.path.isfile(filepath):
            file_time = datetime.fromtimestamp(os.path.getmtime(filepath))
            if file_time < cutoff:
                os.remove(filepath)
                print(f"Supprim√©: {filename}")

# Nettoyer les donn√©es de plus de 30 jours
cleanup_old_files('data/processed', days=30)
cleanup_old_files('data/alerts', days=30)
```

### Lancer le nettoyage

```bash
venv/bin/python scripts/cleanup_old_data.py
```

## üìä Format et encodage

### Sp√©cifications

- **Format** : CSV (Comma-Separated Values)
- **Encodage** : UTF-8
- **S√©parateur** : `,` (virgule)
- **D√©cimales** : `.` (point)
- **Dates** : ISO 8601 (YYYY-MM-DD)
- **Header** : Premi√®re ligne (noms de colonnes)

### Exemple de lecture

```python
import pandas as pd

# Lecture standard
df = pd.read_csv('data/processed/dataset_with_alerts_latest.csv')

# Lecture avec options
df = pd.read_csv(
    'data/processed/dataset_with_alerts_latest.csv',
    encoding='utf-8',
    parse_dates=['date'],
    index_col='date'
)
```

## üîç Validation des donn√©es

### Checks effectu√©s

1. **Compl√©tude** : Toutes les r√©gions pr√©sentes
2. **Coh√©rence** : Valeurs dans les plages attendues
3. **Continuit√©** : Pas de trous dans les dates
4. **Format** : Types de donn√©es corrects

### Script de validation

```python
def validate_data(df):
    # V√©rifier les r√©gions
    expected_regions = 13
    assert df['region'].nunique() == expected_regions

    # V√©rifier les plages de valeurs
    assert (df['vaccination_2024'] >= 0).all()
    assert (df['vaccination_2024'] <= 100).all()
    assert (df['alert_score'] >= 0).all()
    assert (df['alert_score'] <= 100).all()

    print("‚úÖ Donn√©es valides")
```

## üìö R√©f√©rences

- **Donn√©es de d√©monstration** : `scripts/generate_demo_data.py`
- **Pipeline complet** : Voir [INSTALL.md](INSTALL.md)
- **Sources de donn√©es** : Voir [SOURCES.md](SOURCES.md)
- **Structure projet** : Voir [STRUCTURE.md](STRUCTURE.md)
